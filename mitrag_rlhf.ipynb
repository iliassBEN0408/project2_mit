{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPbW9xWLH+V5vRSRvauXGmJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/badrmellal/project2_mit/blob/main/mitrag_rlhf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Project 2: MIT RAG System with RLHF\n",
        "# Author: Badr Mellal & Iliass Benayed\n",
        "# Date: 27 April 2025\n",
        "\n",
        "---\n",
        "\n",
        "Setup & Imports\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "7POoYNSho1ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install -q chromadb sentence-transformers transformers ipywidgets==7.7.1 scikit-learn pandas matplotlib seaborn PyPDF2 python-docx tiktoken\n",
        "\n",
        "# Import libraries\n",
        "import os, re, time, datetime, uuid, json, sqlite3, tempfile\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Setup environment\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.makedirs(\"/content/data/chroma_db\", exist_ok=True)\n",
        "os.makedirs(\"/content/data/feedback\", exist_ok=True)\n",
        "os.makedirs(\"/content/data/models\", exist_ok=True)\n",
        "from google.colab import output, files\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "K5whhRAZo2yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Document Processing Components\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "b6nNCfIXpBWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MitChunker:\n",
        "    def __init__(self, chunk_size=500, chunk_overlap=100):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "\n",
        "    def split_text(self, text):\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        text_length = len(text)\n",
        "\n",
        "        # Adjust chunk size dynamically\n",
        "        if text_length < 5000:\n",
        "            chunk_size = 400\n",
        "        elif text_length < 15000:\n",
        "            chunk_size = 800\n",
        "        else:\n",
        "            chunk_size = 1200\n",
        "        chunk_overlap = self.chunk_overlap\n",
        "\n",
        "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
        "        print(f\"Paragraphs found: {len(paragraphs)}\")\n",
        "\n",
        "        sections = []\n",
        "\n",
        "        # If badly formatted (1 paragraph = whole text), fallback\n",
        "        if len(paragraphs) <= 1:\n",
        "            for i in range(0, len(text), chunk_size - chunk_overlap):\n",
        "                sections.append(text[i:i+chunk_size])\n",
        "            return sections\n",
        "\n",
        "        current_section, current_length = [], 0\n",
        "\n",
        "        for paragraph in paragraphs:\n",
        "            if not paragraph.strip():\n",
        "                continue\n",
        "            para_length = len(paragraph)\n",
        "\n",
        "            if current_length + para_length > chunk_size and current_section:\n",
        "                sections.append(\"\\n\\n\".join(current_section))\n",
        "\n",
        "                # Start new section with overlap\n",
        "                overlap_size, overlap_paragraphs = 0, []\n",
        "                for prev_para in reversed(current_section):\n",
        "                    if overlap_size + len(prev_para) <= chunk_overlap:\n",
        "                        overlap_paragraphs.insert(0, prev_para)\n",
        "                        overlap_size += len(prev_para)\n",
        "                    else:\n",
        "                        break\n",
        "                current_section = overlap_paragraphs\n",
        "                current_length = overlap_size\n",
        "\n",
        "            current_section.append(paragraph)\n",
        "            current_length += para_length\n",
        "\n",
        "        if current_section:\n",
        "            sections.append(\"\\n\\n\".join(current_section))\n",
        "\n",
        "        return sections\n",
        "\n",
        "\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, chunk_size=500, chunk_overlap=100):\n",
        "        self.chunker = MitChunker(chunk_size, chunk_overlap)\n",
        "        self.metadata = {}\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        if not text: return \"\"\n",
        "        text = text.replace('\\xa0', ' ')\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'\\s([.,:;?!])', r'\\1', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def process_pdf(self, file_path):\n",
        "        try:\n",
        "            text, metadata = \"\", {}\n",
        "            with open(file_path, 'rb') as file:\n",
        "                reader = PyPDF2.PdfReader(file)\n",
        "                metadata['pages'] = len(reader.pages)\n",
        "                for page in reader.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text += page_text + \"\\n\\n\"\n",
        "\n",
        "            self.metadata = metadata\n",
        "            text = self.preprocess_text(text)\n",
        "            chunks = self.chunker.split_text(text)\n",
        "            return text, chunks\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing PDF: {e}\")\n",
        "            return \"\", []\n",
        "\n",
        "    def process_docx(self, file_path):\n",
        "        try:\n",
        "            text, metadata = \"\", {}\n",
        "            doc = Document(file_path)\n",
        "            for para in doc.paragraphs:\n",
        "                if para.text.strip():\n",
        "                    text += para.text.strip() + \"\\n\"\n",
        "\n",
        "            self.metadata = metadata\n",
        "            text = self.preprocess_text(text)\n",
        "            chunks = self.chunker.split_text(text)\n",
        "            return text, chunks\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing DOCX: {e}\")\n",
        "            return \"\", []\n",
        "\n",
        "    def process_text(self, file_path):\n",
        "        try:\n",
        "            with open(file_path, 'rb') as file:\n",
        "                file_content = file.read()\n",
        "\n",
        "            text = None\n",
        "            for encoding in ['utf-8', 'latin-1', 'windows-1252']:\n",
        "                try:\n",
        "                    text = file_content.decode(encoding)\n",
        "                    break\n",
        "                except UnicodeDecodeError: continue\n",
        "\n",
        "            if text is None:\n",
        "                raise ValueError(\"Could not decode file with any supported encoding\")\n",
        "\n",
        "            self.metadata = {'size': len(file_content), 'format': 'text', 'path': file_path}\n",
        "            text = self.preprocess_text(text)\n",
        "            chunks = self.chunker.split_text(text)\n",
        "            return text, chunks\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing text file: {e}\")\n",
        "            return \"\", []\n",
        "\n",
        "    def process_document(self, file_path=None, file_content=None, file_name=None):\n",
        "        start_time = time.time()\n",
        "        text, chunks = \"\", []\n",
        "\n",
        "        if file_path is not None:\n",
        "            if not os.path.exists(file_path):\n",
        "                return \"\", []\n",
        "\n",
        "            file_extension = Path(file_path).suffix.lower()\n",
        "            file_name = os.path.basename(file_path)\n",
        "\n",
        "            if file_extension == '.pdf':\n",
        "                text, chunks = self.process_pdf(file_path)\n",
        "            elif file_extension in ['.docx', '.doc']:\n",
        "                text, chunks = self.process_docx(file_path)\n",
        "            elif file_extension in ['.txt', '.md']:\n",
        "                text, chunks = self.process_text(file_path)\n",
        "\n",
        "        elif file_content is not None and file_name:\n",
        "            file_extension = Path(file_name).suffix.lower()\n",
        "\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as temp_file:\n",
        "                temp_file.write(file_content)\n",
        "                temp_path = temp_file.name\n",
        "\n",
        "            if file_extension == '.pdf':\n",
        "                text, chunks = self.process_pdf(temp_path)\n",
        "            elif file_extension in ['.docx', '.doc']:\n",
        "                text, chunks = self.process_docx(temp_path)\n",
        "            elif file_extension in ['.txt', '.md']:\n",
        "                text, chunks = self.process_text(temp_path)\n",
        "\n",
        "            try:\n",
        "                os.unlink(temp_path)\n",
        "            except: pass\n",
        "\n",
        "            self.metadata.update({\"uploaded_filename\": file_name, \"file_size_bytes\": len(file_content)})\n",
        "        else:\n",
        "            return \"\", []\n",
        "\n",
        "        self.metadata.update({\n",
        "            \"filename\": file_name,\n",
        "            \"file_type\": Path(file_name).suffix.lower(),\n",
        "            \"total_chars\": len(text),\n",
        "            \"chunk_count\": len(chunks),\n",
        "            \"processing_timestamp\": datetime.datetime.now().isoformat()\n",
        "        })\n",
        "\n",
        "        return text, chunks"
      ],
      "metadata": {
        "id": "8ehDaz4NpDre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Vector Retrieval System\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WUDlwtHK6-GP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MitRetriever:\n",
        "    def __init__(self, collection_name=\"default_collection\",\n",
        "                 model_name='paraphrase-multilingual-MiniLM-L12-v2',\n",
        "                 persist_directory=\"/content/data/chroma_db\"):\n",
        "        self.model_name = model_name\n",
        "        self.collection_name = collection_name\n",
        "        self.persist_directory = persist_directory\n",
        "        self.document_chunks = []\n",
        "        self.chunk_ids = []\n",
        "\n",
        "        # Initialize ChromaDB\n",
        "        self.chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
        "        try:\n",
        "            self.collection = self.chroma_client.get_collection(name=collection_name)\n",
        "            print(f\"Retrieved existing collection '{collection_name}' with {self.collection.count()} documents\")\n",
        "        except:\n",
        "            self.collection = self.chroma_client.create_collection(name=collection_name)\n",
        "            print(f\"Created new collection '{collection_name}'\")\n",
        "\n",
        "        # Initialize embedding model\n",
        "        print(f\"Loading embedding model: {model_name}\")\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "        # Fallback TF-IDF\n",
        "        self.vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 2))\n",
        "        self.tfidf_matrix = None\n",
        "\n",
        "    def add_documents(self, chunks, metadata=None):\n",
        "        if not chunks: return\n",
        "\n",
        "        self.document_chunks = chunks\n",
        "        self.chunk_ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
        "\n",
        "        if metadata is None:\n",
        "            metadata = [{\"index\": i, \"source\": \"document\"} for i in range(len(chunks))]\n",
        "\n",
        "        # Compute embeddings and add to ChromaDB\n",
        "        print(f\"Computing embeddings for {len(chunks)} chunks...\")\n",
        "        embeddings = self.model.encode(chunks, show_progress_bar=True)\n",
        "        self.collection.add(\n",
        "            embeddings=embeddings.tolist(),\n",
        "            documents=chunks,\n",
        "            ids=self.chunk_ids,\n",
        "            metadatas=metadata\n",
        "        )\n",
        "        print(f\"Added {len(chunks)} chunks to vector database\")\n",
        "\n",
        "        # Also compute TF-IDF as fallback\n",
        "        self.tfidf_matrix = self.vectorizer.fit_transform(chunks)\n",
        "\n",
        "    def search(self, query, top_k=5):\n",
        "        if not self.document_chunks and self.collection.count() == 0:\n",
        "            return []\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # Try ChromaDB with embeddings\n",
        "        try:\n",
        "            query_embedding = self.model.encode(query).tolist()\n",
        "            chroma_results = self.collection.query(\n",
        "                query_embeddings=query_embedding,\n",
        "                n_results=top_k,\n",
        "                include=[\"documents\", \"distances\", \"metadatas\"]\n",
        "            )\n",
        "\n",
        "            if chroma_results and 'documents' in chroma_results and len(chroma_results['documents']) > 0:\n",
        "                documents = chroma_results['documents'][0]\n",
        "                distances = chroma_results.get('distances', [[]])[0]\n",
        "                metadatas = chroma_results.get('metadatas', [[]])[0]\n",
        "\n",
        "                for i, (doc, distance) in enumerate(zip(documents, distances)):\n",
        "                    metadata = metadatas[i] if i < len(metadatas) else {}\n",
        "                    score = 1.0 - distance if distance < 1.0 else 0.1\n",
        "\n",
        "                    results.append({\n",
        "                        'rank': i + 1,\n",
        "                        'index': metadata.get('index', i),\n",
        "                        'score': score,\n",
        "                        'text': doc,\n",
        "                        'method': 'embedding',\n",
        "                        'metadata': metadata\n",
        "                    })\n",
        "\n",
        "                return results\n",
        "        except Exception as e:\n",
        "            print(f\"Error in embedding search: {e}\")\n",
        "\n",
        "        # Fallback to TF-IDF if needed\n",
        "        if not results and self.tfidf_matrix is not None:\n",
        "            query_vector = self.vectorizer.transform([query])\n",
        "            similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
        "            top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "            for i, idx in enumerate(top_indices):\n",
        "                score = float(similarities[idx])\n",
        "                if score > 0:\n",
        "                    results.append({\n",
        "                        'rank': i + 1,\n",
        "                        'index': int(idx),\n",
        "                        'score': score,\n",
        "                        'text': self.document_chunks[idx],\n",
        "                        'method': 'tfidf'\n",
        "                    })\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "ZzipuAum63XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "LLM Integration\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "CAZjkEvDpTUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HuggingFaceLLM:\n",
        "    def __init__(self, model_name=\"google/flan-t5-base\"):\n",
        "        self.model_name = model_name\n",
        "        try:\n",
        "            print(f\"Loading LLM: {model_name}...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "            self.generator = pipeline(\"text2text-generation\", model=self.model, tokenizer=self.tokenizer, max_length=512)\n",
        "            self.available = True\n",
        "            print(f\"Successfully loaded {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            self.available = False\n",
        "\n",
        "    def generate_answer(self, query, contexts):\n",
        "        if not contexts:\n",
        "            return \"No relevant information found to answer this question.\"\n",
        "\n",
        "        # Prepare context\n",
        "        context_text = \"\\n\\n\".join([c['text'] for c in contexts])\n",
        "\n",
        "        # Generate answer with model if available\n",
        "        if self.available:\n",
        "            try:\n",
        "                # Prepare prompt - we keep it simple for T5 models\n",
        "                prompt = f\"Answer based on this context: {context_text[:1500]}\\n\\nQuestion: {query}\"\n",
        "\n",
        "                # Generate answer\n",
        "                outputs = self.generator(prompt, max_length=300, do_sample=True, temperature=0.7)\n",
        "                answer = outputs[0]['generated_text']\n",
        "\n",
        "                # Format answer\n",
        "                if len(answer.split()) < 5:  # If too short, use extraction\n",
        "                    return self._extract_answer(query, contexts)\n",
        "\n",
        "                return f\"Based on the document information:\\n\\n{answer}\"\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating with model: {e}\")\n",
        "                return self._extract_answer(query, contexts)\n",
        "        else:\n",
        "            return self._extract_answer(query, contexts)\n",
        "\n",
        "    def _extract_answer(self, query, contexts):\n",
        "        # Simple extraction fallback\n",
        "        if not contexts:\n",
        "            return \"No relevant information found.\"\n",
        "\n",
        "        # Get the most relevant passages\n",
        "        best_context = contexts[0]['text']\n",
        "\n",
        "        # Find sentences that might answer the query\n",
        "        all_text = \" \".join([c['text'] for c in contexts[:3]])\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', all_text)\n",
        "\n",
        "        # Remove common words from query\n",
        "        stop_words = {'the', 'and', 'is', 'in', 'it', 'to', 'of', 'for', 'a', 'on', 'with', 'what', 'how', 'why'}\n",
        "        query_words = set(word.lower() for word in query.split() if word.lower() not in stop_words)\n",
        "\n",
        "        # Score sentences by relevance to query\n",
        "        best_sentences = []\n",
        "        for sentence in sentences:\n",
        "            if not sentence.strip(): continue\n",
        "            sentence_words = set(word.lower() for word in sentence.split())\n",
        "            matching_words = sentence_words.intersection(query_words)\n",
        "            if matching_words:\n",
        "                best_sentences.append((len(matching_words), sentence))\n",
        "\n",
        "        # Sort by relevance\n",
        "        best_sentences.sort(reverse=True)\n",
        "\n",
        "        if best_sentences:\n",
        "            answer_sentences = [s[1] for s in best_sentences[:3]]\n",
        "            return f\"Based on the document information:\\n\\n{' '.join(answer_sentences)}\"\n",
        "        else:\n",
        "            return f\"Based on the document information:\\n\\n{best_context}\""
      ],
      "metadata": {
        "id": "tiyFxepEpX_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Feedback System\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "rWxgtNxp2JOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedbackSystem:\n",
        "    def __init__(self, db_path=\"/content/data/feedback/feedback.db\", model_path=\"/content/data/models\"):\n",
        "        self.db_path = db_path\n",
        "        self.model_path = model_path\n",
        "        os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
        "        os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "        # Initialize database\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute('''\n",
        "        CREATE TABLE IF NOT EXISTS feedback (\n",
        "            id TEXT PRIMARY KEY,\n",
        "            query TEXT,\n",
        "            answer TEXT,\n",
        "            context TEXT,\n",
        "            rating INTEGER,\n",
        "            timestamp TEXT,\n",
        "            model TEXT,\n",
        "            retrieval_method TEXT,\n",
        "            metadata TEXT\n",
        "        )\n",
        "        ''')\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "        # RLHF components\n",
        "        self.reward_model = None\n",
        "        self.reward_tokenizer = None\n",
        "        self.rlhf_model = None\n",
        "        self.rlhf_tokenizer = None\n",
        "\n",
        "        # Configuration\n",
        "        self.min_samples = 10  # Minimum samples for RLHF training\n",
        "        self.reward_model_name = \"distilbert-base-uncased\"  # Base model for reward function\n",
        "        self.reward_model_path = os.path.join(model_path, \"reward_model\")\n",
        "        self.rlhf_model_path = os.path.join(model_path, \"rlhf_model\")\n",
        "\n",
        "    def add_feedback(self, query, answer, context, rating, model=\"unknown\", retrieval_method=\"unknown\", metadata=None):\n",
        "        try:\n",
        "            feedback_id = str(uuid.uuid4())\n",
        "            timestamp = datetime.datetime.now().isoformat()\n",
        "            metadata_json = json.dumps(metadata) if metadata else \"{}\"\n",
        "\n",
        "            if isinstance(context, (list, dict)):\n",
        "                context = json.dumps(context)\n",
        "\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute('''\n",
        "            INSERT INTO feedback (id, query, answer, context, rating, timestamp, model, retrieval_method, metadata)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            ''', (feedback_id, query, answer, context, rating, timestamp, model, retrieval_method, metadata_json))\n",
        "            conn.commit()\n",
        "            conn.close()\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error storing feedback: {e}\")\n",
        "            return False\n",
        "\n",
        "    def get_feedback_stats(self):\n",
        "        try:\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            cursor.execute(\"SELECT COUNT(*) FROM feedback\")\n",
        "            total_count = cursor.fetchone()[0]\n",
        "\n",
        "            cursor.execute(\"SELECT AVG(rating) FROM feedback\")\n",
        "            avg_rating = cursor.fetchone()[0]\n",
        "\n",
        "            cursor.execute(\"SELECT rating, COUNT(*) FROM feedback GROUP BY rating\")\n",
        "            rating_dist = dict(cursor.fetchall())\n",
        "\n",
        "            conn.close()\n",
        "\n",
        "            return {\n",
        "                \"total_feedback\": total_count,\n",
        "                \"average_rating\": avg_rating,\n",
        "                \"rating_distribution\": rating_dist\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting feedback stats: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def analyze_feedback(self):\n",
        "        stats = self.get_feedback_stats()\n",
        "        if not stats or stats.get(\"total_feedback\", 0) == 0:\n",
        "            return {\"status\": \"No data\"}\n",
        "\n",
        "        try:\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            cursor.execute(\"SELECT query, answer, context, rating FROM feedback WHERE rating >= 4\")\n",
        "            high_rated = cursor.fetchall()\n",
        "\n",
        "            cursor.execute(\"SELECT query, answer, context, rating FROM feedback WHERE rating <= 2\")\n",
        "            low_rated = cursor.fetchall()\n",
        "\n",
        "            conn.close()\n",
        "\n",
        "            # Extract insights\n",
        "            insights = []\n",
        "\n",
        "            # Add default insights with limited data\n",
        "            if len(high_rated) < 3 or len(low_rated) < 3:\n",
        "                insights.append(\"Not enough feedback data for detailed analysis\")\n",
        "                if len(high_rated) > len(low_rated):\n",
        "                    insights.append(\"Users seem to prefer more detailed answers\")\n",
        "                if len(low_rated) > len(high_rated):\n",
        "                    insights.append(\"Users prefer more concise answers\")\n",
        "            else:\n",
        "                # Compare answer length\n",
        "                high_lengths = [len(answer) for _, answer, _, _ in high_rated]\n",
        "                low_lengths = [len(answer) for _, answer, _, _ in low_rated]\n",
        "                avg_high = sum(high_lengths) / len(high_lengths)\n",
        "                avg_low = sum(low_lengths) / len(low_lengths)\n",
        "\n",
        "                if avg_high > avg_low * 1.2:\n",
        "                    insights.append(\"Longer answers tend to receive higher ratings\")\n",
        "                elif avg_low > avg_high * 1.2:\n",
        "                    insights.append(\"Shorter, more concise answers tend to receive higher ratings\")\n",
        "\n",
        "            return {\n",
        "                \"total_feedback\": stats[\"total_feedback\"],\n",
        "                \"average_rating\": stats[\"average_rating\"],\n",
        "                \"high_rated_count\": len(high_rated),\n",
        "                \"low_rated_count\": len(low_rated),\n",
        "                \"insights\": insights\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing feedback: {e}\")\n",
        "            return {\"status\": \"Error\", \"message\": str(e)}\n",
        "\n",
        "    # ----- Neural Network RLHF Methods -----\n",
        "\n",
        "    def initialize_reward_model(self):\n",
        "        \"\"\"Initialize a neural network reward model for predicting ratings\"\"\"\n",
        "        try:\n",
        "            # Check if we have a saved model\n",
        "            if os.path.exists(self.reward_model_path):\n",
        "                print(f\"Loading existing reward model from {self.reward_model_path}\")\n",
        "                self.reward_tokenizer = AutoTokenizer.from_pretrained(self.reward_model_path)\n",
        "                self.reward_model = AutoModelForSequenceClassification.from_pretrained(self.reward_model_path)\n",
        "            else:\n",
        "                print(f\"Initializing new reward model from {self.reward_model_name}\")\n",
        "                self.reward_tokenizer = AutoTokenizer.from_pretrained(self.reward_model_name)\n",
        "                self.reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                    self.reward_model_name,\n",
        "                    num_labels=1  # Regression model for rating prediction\n",
        "                )\n",
        "\n",
        "            # Move model to GPU if available\n",
        "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            self.reward_model.to(self.device)\n",
        "\n",
        "            print(f\"Reward model initialized on {self.device}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing reward model: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return False\n",
        "\n",
        "    class FeedbackDataset(Dataset):\n",
        "        \"\"\"Neural network dataset for feedback data\"\"\"\n",
        "        def __init__(self, data, tokenizer, max_length=512):\n",
        "            self.data = data\n",
        "            self.tokenizer = tokenizer\n",
        "            self.max_length = max_length\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.data)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            query, answer, rating = self.data[idx]\n",
        "            inputs = self.tokenizer(\n",
        "                f\"Question: {query} Answer: {answer}\",\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=self.max_length,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
        "                \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
        "                \"rating\": torch.tensor(rating, dtype=torch.float)\n",
        "            }\n",
        "\n",
        "    def train_reward_model(self):\n",
        "        \"\"\"Train neural network reward model on collected feedback\"\"\"\n",
        "        if not self.initialize_reward_model():\n",
        "            return False\n",
        "\n",
        "        # Check if we have enough data\n",
        "        stats = self.get_feedback_stats()\n",
        "        if stats.get(\"total_feedback\", 0) < self.min_samples:\n",
        "            print(f\"Not enough feedback data for training. Need at least {self.min_samples}, have {stats.get('total_feedback', 0)}\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Get the feedback data\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute(\"SELECT query, answer, rating FROM feedback\")\n",
        "            feedback_data = cursor.fetchall()\n",
        "            conn.close()\n",
        "\n",
        "            print(f\"Training reward model on {len(feedback_data)} feedback samples\")\n",
        "\n",
        "            # Create dataset and dataloader\n",
        "            dataset = self.FeedbackDataset(feedback_data, self.reward_tokenizer)\n",
        "            dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "            # Set up training\n",
        "            optimizer = torch.optim.AdamW(self.reward_model.parameters(), lr=2e-5)\n",
        "            loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "            # Training loop\n",
        "            self.reward_model.train()\n",
        "            for epoch in range(3):  # 3 epochs is reasonable for small datasets\n",
        "                total_loss = 0\n",
        "                for batch in dataloader:\n",
        "                    # Move batch to device\n",
        "                    input_ids = batch[\"input_ids\"].to(self.device)\n",
        "                    attention_mask = batch[\"attention_mask\"].to(self.device)\n",
        "                    ratings = batch[\"rating\"].to(self.device)\n",
        "\n",
        "                    # Forward pass\n",
        "                    self.reward_model.zero_grad()\n",
        "                    outputs = self.reward_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                    logits = outputs.logits.squeeze()\n",
        "\n",
        "                    # Calculate loss and backpropagate\n",
        "                    loss = loss_fn(logits, ratings)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "                avg_loss = total_loss / len(dataloader)\n",
        "                print(f\"Epoch {epoch+1}/3, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "            # Save the trained reward model\n",
        "            os.makedirs(self.reward_model_path, exist_ok=True)\n",
        "            self.reward_model.save_pretrained(self.reward_model_path)\n",
        "            self.reward_tokenizer.save_pretrained(self.reward_model_path)\n",
        "            print(f\"Reward model saved to {self.reward_model_path}\")\n",
        "\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error training reward model: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return False\n",
        "\n",
        "    def predict_reward(self, query, answer):\n",
        "        \"\"\"Use neural network to predict the reward/rating for a query-answer pair\"\"\"\n",
        "        if self.reward_model is None or self.reward_tokenizer is None:\n",
        "            if not self.initialize_reward_model():\n",
        "                return 3.0  # Default neutral rating\n",
        "\n",
        "        try:\n",
        "            # Tokenize the input\n",
        "            inputs = self.reward_tokenizer(\n",
        "                f\"Question: {query} Answer: {answer}\",\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(self.device)\n",
        "\n",
        "            # Get prediction from model\n",
        "            self.reward_model.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = self.reward_model(**inputs)\n",
        "                predicted_reward = outputs.logits.item()\n",
        "\n",
        "            # Clamp to valid rating range (1-5)\n",
        "            return max(1.0, min(5.0, predicted_reward))\n",
        "        except Exception as e:\n",
        "            print(f\"Error predicting reward: {e}\")\n",
        "            return 3.0  # Default neutral rating\n",
        "\n",
        "    class RLHFDataset(Dataset):\n",
        "        \"\"\"Dataset for RLHF fine-tuning\"\"\"\n",
        "        def __init__(self, data, tokenizer, max_length=512, max_target_length=128):\n",
        "            self.data = data\n",
        "            self.tokenizer = tokenizer\n",
        "            self.max_length = max_length\n",
        "            self.max_target_length = max_target_length\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.data)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            query, context, answer, rating = self.data[idx]\n",
        "\n",
        "            # Process context\n",
        "            if isinstance(context, str) and (context.startswith('[') or context.startswith('{')):\n",
        "                try:\n",
        "                    # Try to parse as JSON\n",
        "                    context_obj = json.loads(context)\n",
        "                    if isinstance(context_obj, list):\n",
        "                        context = \" \".join([c[:300] for c in context_obj[:2]])  # Use first 2 contexts\n",
        "                    else:\n",
        "                        context = str(context_obj)[:500]  # Truncate if dict or other\n",
        "                except:\n",
        "                    # If JSON parsing fails, use as text but truncate\n",
        "                    context = context[:500]\n",
        "\n",
        "            # Create input prompt\n",
        "            input_text = f\"Context: {context[:500]} Question: {query}\"\n",
        "\n",
        "            # Tokenize input and target\n",
        "            model_inputs = self.tokenizer(\n",
        "                input_text,\n",
        "                max_length=self.max_length,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            with self.tokenizer.as_target_tokenizer():\n",
        "                labels = self.tokenizer(\n",
        "                    answer,\n",
        "                    max_length=self.max_target_length,\n",
        "                    padding=\"max_length\",\n",
        "                    truncation=True,\n",
        "                    return_tensors=\"pt\"\n",
        "                )\n",
        "\n",
        "            # For T5-like models, replace padding token id in labels\n",
        "            labels[\"input_ids\"][labels[\"input_ids\"] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "            # Create weight based on rating (higher ratings = higher weights)\n",
        "            weight = rating / 3.0  # Scale ratings\n",
        "\n",
        "            return {\n",
        "                \"input_ids\": model_inputs[\"input_ids\"].squeeze(),\n",
        "                \"attention_mask\": model_inputs[\"attention_mask\"].squeeze(),\n",
        "                \"labels\": labels[\"input_ids\"].squeeze(),\n",
        "                \"weight\": torch.tensor(weight, dtype=torch.float)\n",
        "            }\n",
        "\n",
        "    class PPOTrainer:\n",
        "        \"\"\"Simplified PPO trainer for RLHF\"\"\"\n",
        "        def __init__(self, model, ref_model, tokenizer, reward_function, device):\n",
        "            self.model = model\n",
        "            self.ref_model = ref_model\n",
        "            self.tokenizer = tokenizer\n",
        "            self.reward_function = reward_function\n",
        "            self.device = device\n",
        "\n",
        "            # PPO parameters\n",
        "            self.gamma = 0.99\n",
        "            self.clip_param = 0.2\n",
        "            self.value_loss_coef = 0.5\n",
        "            self.entropy_coef = 0.01\n",
        "            self.max_grad_norm = 0.5\n",
        "\n",
        "            # Initialize optimizer\n",
        "            self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=5e-6)\n",
        "\n",
        "        def generate_response(self, context, query):\n",
        "            \"\"\"Generate a response from the model\"\"\"\n",
        "            prompt = f\"Context: {context} Question: {query}\"\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
        "\n",
        "            # Generate output\n",
        "            with torch.no_grad():\n",
        "                output_ids = self.model.generate(\n",
        "                    inputs[\"input_ids\"],\n",
        "                    max_length=128,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.8,\n",
        "                    return_dict_in_generate=True,\n",
        "                    output_scores=True\n",
        "                )\n",
        "\n",
        "            # Decode the response\n",
        "            response = self.tokenizer.decode(output_ids.sequences[0], skip_special_tokens=True)\n",
        "            return response\n",
        "\n",
        "        def compute_rewards(self, queries, contexts, responses):\n",
        "            \"\"\"Compute rewards using the reward model\"\"\"\n",
        "            rewards = []\n",
        "            for query, response in zip(queries, responses):\n",
        "                reward = self.reward_function(query, response)\n",
        "                rewards.append(reward)\n",
        "            return torch.tensor(rewards, device=self.device)\n",
        "\n",
        "        def train_step(self, context, query, response, reward):\n",
        "            \"\"\"Single PPO training step\"\"\"\n",
        "            # Prepare input\n",
        "            prompt = f\"Context: {context} Question: {query}\"\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "            # Prepare target\n",
        "            with self.tokenizer.as_target_tokenizer():\n",
        "                target = self.tokenizer(response, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "            # Get log probabilities from current model\n",
        "            outputs = self.model(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                labels=target[\"input_ids\"]\n",
        "            )\n",
        "            current_log_prob = -outputs.loss\n",
        "\n",
        "            # Get log probabilities from reference model (for KL penalty)\n",
        "            with torch.no_grad():\n",
        "                ref_outputs = self.ref_model(\n",
        "                    input_ids=inputs[\"input_ids\"],\n",
        "                    attention_mask=inputs[\"attention_mask\"],\n",
        "                    labels=target[\"input_ids\"]\n",
        "                )\n",
        "                ref_log_prob = -ref_outputs.loss\n",
        "\n",
        "            # Calculate KL divergence\n",
        "            kl_div = current_log_prob - ref_log_prob\n",
        "\n",
        "            # Calculate PPO loss\n",
        "            advantage = reward - 3.0  # Baseline is average rating\n",
        "            ratio = torch.exp(current_log_prob - ref_log_prob)\n",
        "            surr1 = ratio * advantage\n",
        "            surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * advantage\n",
        "\n",
        "            # Compute final loss\n",
        "            policy_loss = -torch.min(surr1, surr2)\n",
        "            kl_coef = 0.05  # KL penalty coefficient\n",
        "            loss = policy_loss + kl_coef * kl_div\n",
        "\n",
        "            # Backpropagation\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            return loss.item()\n",
        "\n",
        "    def train_rlhf_model(self, model_name=\"google/flan-t5-base\"):\n",
        "        \"\"\"Train a model with RLHF using PPO\"\"\"\n",
        "        # First make sure reward model is trained\n",
        "        if self.reward_model is None:\n",
        "            if not self.train_reward_model():\n",
        "                return False\n",
        "\n",
        "        # Check if we have enough data\n",
        "        stats = self.get_feedback_stats()\n",
        "        if stats.get(\"total_feedback\", 0) < self.min_samples:\n",
        "            print(f\"Not enough feedback data for RLHF. Need at least {self.min_samples}, have {stats.get('total_feedback', 0)}\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Get the feedback data\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute(\"SELECT query, context, answer, rating FROM feedback\")\n",
        "            feedback_data = cursor.fetchall()\n",
        "            conn.close()\n",
        "\n",
        "            print(f\"Training RLHF model on {len(feedback_data)} feedback samples\")\n",
        "\n",
        "            # Load models\n",
        "            print(f\"Loading base model: {model_name}\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "            # Create a reference model (frozen copy)\n",
        "            ref_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "            # Move models to device\n",
        "            device = self.device\n",
        "            model.to(device)\n",
        "            ref_model.to(device)\n",
        "\n",
        "            # Create PPO trainer\n",
        "            ppo_trainer = self.PPOTrainer(\n",
        "                model=model,\n",
        "                ref_model=ref_model,\n",
        "                tokenizer=tokenizer,\n",
        "                reward_function=self.predict_reward,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            # First phase: supervised fine-tuning on human feedback\n",
        "            # This helps align the model before PPO\n",
        "\n",
        "            print(\"Phase 1: Supervised fine-tuning on high-rated examples\")\n",
        "\n",
        "            # Create dataset from feedback (weighted by ratings)\n",
        "            dataset = self.RLHFDataset(feedback_data, tokenizer)\n",
        "            dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "            # Training parameters\n",
        "            optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "            model.train()\n",
        "\n",
        "            # Train supervised model first\n",
        "            for epoch in range(2):  # 2 epochs is enough for initial alignment\n",
        "                total_loss = 0\n",
        "                for batch in dataloader:\n",
        "                    # Move data to device\n",
        "                    input_ids = batch[\"input_ids\"].to(device)\n",
        "                    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                    labels = batch[\"labels\"].to(device)\n",
        "                    weights = batch[\"weight\"].to(device)\n",
        "\n",
        "                    # Forward pass\n",
        "                    outputs = model(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        labels=labels\n",
        "                    )\n",
        "\n",
        "                    # Weight loss by rating (higher rated examples have more influence)\n",
        "                    loss = outputs.loss * weights\n",
        "                    loss = loss.mean()\n",
        "\n",
        "                    # Backpropagate\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "                avg_loss = total_loss / len(dataloader)\n",
        "                print(f\"SFT Epoch {epoch+1}/2, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "            print(\"Phase 2: PPO fine-tuning\")\n",
        "\n",
        "            # PPO training loop\n",
        "            num_ppo_epochs = 3\n",
        "            for epoch in range(num_ppo_epochs):\n",
        "                total_loss = 0\n",
        "                batch_count = 0\n",
        "\n",
        "                # Generate new responses for all examples\n",
        "                for idx, (query, context, _, _) in enumerate(feedback_data):\n",
        "                    if isinstance(context, str) and (context.startswith('[') or context.startswith('{')):\n",
        "                        try:\n",
        "                            context_obj = json.loads(context)\n",
        "                            if isinstance(context_obj, list):\n",
        "                                context = \" \".join([str(c)[:200] for c in context_obj[:2]])\n",
        "                            else:\n",
        "                                context = str(context_obj)[:400]\n",
        "                        except:\n",
        "                            context = context[:400]\n",
        "\n",
        "                    # Generate a response\n",
        "                    response = ppo_trainer.generate_response(context, query)\n",
        "\n",
        "                    # Calculate reward\n",
        "                    reward = self.predict_reward(query, response)\n",
        "\n",
        "                    # Update policy with PPO\n",
        "                    loss = ppo_trainer.train_step(context, query, response, reward)\n",
        "                    total_loss += loss\n",
        "                    batch_count += 1\n",
        "\n",
        "                    # Periodically print progress\n",
        "                    if (idx + 1) % 5 == 0 or idx == len(feedback_data) - 1:\n",
        "                        print(f\"PPO Epoch {epoch+1}/{num_ppo_epochs}, Sample {idx+1}/{len(feedback_data)}, Loss: {total_loss/batch_count:.4f}\")\n",
        "\n",
        "            # Save the fine-tuned model\n",
        "            os.makedirs(self.rlhf_model_path, exist_ok=True)\n",
        "            model.save_pretrained(self.rlhf_model_path)\n",
        "            tokenizer.save_pretrained(self.rlhf_model_path)\n",
        "\n",
        "            print(f\"RLHF model saved to {self.rlhf_model_path}\")\n",
        "\n",
        "            # Save for inference\n",
        "            self.rlhf_model = model\n",
        "            self.rlhf_tokenizer = tokenizer\n",
        "\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error training RLHF model: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return False\n",
        "\n",
        "    def initialize_rlhf_model(self):\n",
        "        \"\"\"Load the fine-tuned RLHF model if available\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.rlhf_model_path):\n",
        "                print(f\"Loading existing RLHF model from {self.rlhf_model_path}\")\n",
        "                self.rlhf_tokenizer = AutoTokenizer.from_pretrained(self.rlhf_model_path)\n",
        "                self.rlhf_model = AutoModelForSeq2SeqLM.from_pretrained(self.rlhf_model_path)\n",
        "                self.rlhf_model.to(self.device)\n",
        "                return True\n",
        "            else:\n",
        "                print(\"No RLHF model found. Train model first with train_rlhf_model()\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading RLHF model: {e}\")\n",
        "            return False\n",
        "\n",
        "    def generate_rlhf_answer(self, query, contexts, max_length=200):\n",
        "        \"\"\"Generate an answer using the RLHF model\"\"\"\n",
        "        if self.rlhf_model is None or self.rlhf_tokenizer is None:\n",
        "            if not self.initialize_rlhf_model():\n",
        "                return None\n",
        "\n",
        "        try:\n",
        "            # Prepare context\n",
        "            if isinstance(contexts, list):\n",
        "                if isinstance(contexts[0], dict) and 'text' in contexts[0]:\n",
        "                    context_text = \" \".join([c['text'][:300] for c in contexts[:2]])\n",
        "                else:\n",
        "                    context_text = \" \".join([str(c)[:300] for c in contexts[:2]])\n",
        "            else:\n",
        "                context_text = str(contexts)[:500]\n",
        "\n",
        "            # Create prompt\n",
        "            prompt = f\"Context: {context_text} Question: {query}\"\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = self.rlhf_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
        "\n",
        "            # Generate answer\n",
        "            self.rlhf_model.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = self.rlhf_model.generate(\n",
        "                    inputs[\"input_ids\"],\n",
        "                    max_length=max_length,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.7,\n",
        "                    num_return_sequences=1\n",
        "                )\n",
        "\n",
        "            # Decode the answer\n",
        "            answer = self.rlhf_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            return answer\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating with RLHF model: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def run_rlhf_training_pipeline(self):\n",
        "        \"\"\"Run the complete RLHF pipeline\"\"\"\n",
        "        print(\"Starting RLHF training pipeline...\")\n",
        "\n",
        "        # Step 1: Initialize and train reward model\n",
        "        print(\"\\n1. Training reward model...\")\n",
        "        if not self.train_reward_model():\n",
        "            return False\n",
        "\n",
        "        # Step 2: Train RLHF model with PPO\n",
        "        print(\"\\n2. Training RLHF model with PPO...\")\n",
        "        if not self.train_rlhf_model():\n",
        "            return False\n",
        "\n",
        "        print(\"\\nRLHF training pipeline completed successfully!\")\n",
        "        return True\n",
        "\n",
        "\n",
        "# Enhancing LLM with RLHF support\n",
        "class RLHFEnhancingLLM(HuggingFaceLLM):\n",
        "    def __init__(self, model_name=\"google/flan-t5-base\", feedback_system=None):\n",
        "        super().__init__(model_name)\n",
        "        self.feedback_system = feedback_system\n",
        "        self.use_rlhf = False\n",
        "\n",
        "    def toggle_rlhf(self, enable=True):\n",
        "        \"\"\"Toggle between RLHF and base model\"\"\"\n",
        "        self.use_rlhf = enable\n",
        "        return self.use_rlhf\n",
        "\n",
        "    def generate_answer(self, query, contexts):\n",
        "        \"\"\"Generate answer using RLHF if enabled, otherwise use base model\"\"\"\n",
        "        if not contexts:\n",
        "            return \"No relevant information found to answer this question.\"\n",
        "\n",
        "        # If RLHF is enabled and the feedback system is available\n",
        "        if self.use_rlhf and self.feedback_system is not None:\n",
        "            # Try to generate answer with RLHF model\n",
        "            rlhf_answer = self.feedback_system.generate_rlhf_answer(query, contexts)\n",
        "\n",
        "            if rlhf_answer:\n",
        "                return f\"Based on the document information (RLHF):\\n\\n{rlhf_answer}\"\n",
        "\n",
        "        # Fall back to base model\n",
        "        return super().generate_answer(query, contexts)"
      ],
      "metadata": {
        "id": "O1CjxEec2My5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Main RAG System\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "YK6AkSYupaoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MitRAGSystem:\n",
        "    def __init__(self, collection_name=\"default_collection\", chunk_size=500, chunk_overlap=100,\n",
        "                model_name=\"google/flan-t5-base\", embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "                persist_directory=\"/content/data/chroma_db\"):\n",
        "        # Initialize feedback system first\n",
        "        self.feedback = FeedbackSystem()\n",
        "\n",
        "        # Initialize components\n",
        "        self.document_processor = DocumentProcessor(chunk_size, chunk_overlap)\n",
        "        self.retriever = MitRetriever(collection_name, embedding_model, persist_directory)\n",
        "\n",
        "        # Use enhanced LLM with RLHF\n",
        "        self.llm = RLHFEnhancingLLM(model_name, self.feedback)\n",
        "\n",
        "        # Document information\n",
        "        self.document_text = \"\"\n",
        "        self.document_chunks = []\n",
        "        self.document_name = None\n",
        "        self.document_metadata = {}\n",
        "\n",
        "        # Conversation tracking\n",
        "        self.conversation_history = []\n",
        "\n",
        "        # System metrics\n",
        "        self.metrics = {\n",
        "            'document_processing': [],\n",
        "            'queries': [],\n",
        "            'feedback': []\n",
        "        }\n",
        "\n",
        "        # RLHF status\n",
        "        self.rlhf_trained = False\n",
        "\n",
        "        print(f\"MIT RAG System initialized with LLM: {model_name} and Embedding: {embedding_model}\")\n",
        "\n",
        "    def process_document(self, file_path=None, file_content=None, file_name=None):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Process document\n",
        "        text, chunks = self.document_processor.process_document(\n",
        "            file_path=file_path,\n",
        "            file_content=file_content,\n",
        "            file_name=file_name\n",
        "        )\n",
        "\n",
        "        if not chunks:\n",
        "            print(\"No chunks produced from document\")\n",
        "            return False\n",
        "\n",
        "        # Store document info\n",
        "        self.document_text = text\n",
        "        self.document_chunks = chunks\n",
        "        self.document_name = file_name or (file_path and os.path.basename(file_path))\n",
        "        self.document_metadata = self.document_processor.metadata\n",
        "\n",
        "        # Prepare chunk metadata\n",
        "        chunk_metadata = [\n",
        "            {\n",
        "                \"document_name\": self.document_name,\n",
        "                \"chunk_index\": i,\n",
        "                \"document_type\": self.document_metadata.get(\"file_type\", \"unknown\"),\n",
        "                \"source\": \"document\"\n",
        "            } for i in range(len(chunks))\n",
        "        ]\n",
        "\n",
        "        # Add to retriever\n",
        "        self.retriever.add_documents(chunks, metadata=chunk_metadata)\n",
        "\n",
        "        # Track metrics\n",
        "        processing_time = time.time() - start_time\n",
        "        self.metrics['document_processing'].append({\n",
        "            'filename': self.document_name,\n",
        "            'time': processing_time,\n",
        "            'chunks': len(chunks)\n",
        "        })\n",
        "\n",
        "        print(f\"Document processed: {len(chunks)} chunks in {processing_time:.2f}s\")\n",
        "        return True\n",
        "\n",
        "    def answer_question(self, query, top_k=3, use_rlhf=None):\n",
        "        if not self.document_chunks and not hasattr(self.retriever, 'collection'):\n",
        "            return \"Please process a document first.\", [], None\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Set RLHF flag if specified\n",
        "        if use_rlhf is not None:\n",
        "            self.llm.toggle_rlhf(use_rlhf)\n",
        "\n",
        "        # Retrieve relevant passages\n",
        "        retrieval_results = self.retriever.search(query, top_k=top_k)\n",
        "\n",
        "        if not retrieval_results:\n",
        "            return \"No relevant information found to answer this question.\", [], None\n",
        "\n",
        "        # Generate answer\n",
        "        answer = self.llm.generate_answer(query, retrieval_results)\n",
        "\n",
        "        # Create conversation entry\n",
        "        conversation_id = str(uuid.uuid4())\n",
        "        self.conversation_history.append({\n",
        "            \"id\": conversation_id,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"query\": query,\n",
        "            \"answer\": answer,\n",
        "            \"contexts\": retrieval_results,\n",
        "            \"model\": self.llm.model_name,\n",
        "            \"rlhf_used\": self.llm.use_rlhf,\n",
        "            \"feedback\": None\n",
        "        })\n",
        "\n",
        "        # Track metrics\n",
        "        total_time = time.time() - start_time\n",
        "        self.metrics['queries'].append({\n",
        "            'query': query,\n",
        "            'time': total_time,\n",
        "            'result_count': len(retrieval_results),\n",
        "            'rlhf_used': self.llm.use_rlhf\n",
        "        })\n",
        "\n",
        "        return answer, retrieval_results, conversation_id\n",
        "\n",
        "    def add_feedback(self, conversation_id, rating):\n",
        "        # Find conversation entry\n",
        "        entry = next((item for item in self.conversation_history if item[\"id\"] == conversation_id), None)\n",
        "\n",
        "        if not entry:\n",
        "            print(f\"Conversation ID {conversation_id} not found\")\n",
        "            return False\n",
        "\n",
        "        # Update entry with feedback\n",
        "        entry[\"feedback\"] = {\n",
        "            \"rating\": rating,\n",
        "            \"timestamp\": datetime.datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        # Store in feedback database\n",
        "        success = self.feedback.add_feedback(\n",
        "            query=entry[\"query\"],\n",
        "            answer=entry[\"answer\"],\n",
        "            context=[c[\"text\"] for c in entry[\"contexts\"]],\n",
        "            rating=rating,\n",
        "            model=entry[\"model\"],\n",
        "            retrieval_method=entry[\"contexts\"][0][\"method\"] if entry[\"contexts\"] else \"unknown\",\n",
        "            metadata={\"conversation_id\": entry[\"id\"], \"document_name\": self.document_name, \"rlhf_used\": entry.get(\"rlhf_used\", False)}\n",
        "        )\n",
        "\n",
        "        # Track metrics\n",
        "        self.metrics['feedback'].append({\n",
        "            'conversation_id': entry[\"id\"],\n",
        "            'query': entry[\"query\"],\n",
        "            'rating': rating,\n",
        "            'rlhf_used': entry.get(\"rlhf_used\", False)\n",
        "        })\n",
        "\n",
        "        return success\n",
        "\n",
        "    def get_document_info(self):\n",
        "        if not self.document_name:\n",
        "            return {\"status\": \"No document processed\"}\n",
        "\n",
        "        return {\n",
        "            \"filename\": self.document_name,\n",
        "            \"chunks\": len(self.document_chunks),\n",
        "            \"total_characters\": len(self.document_text),\n",
        "            **{k: v for k, v in self.document_metadata.items()\n",
        "               if k not in ['processing_timestamp']}\n",
        "        }\n",
        "\n",
        "    def analyze_feedback(self):\n",
        "        return self.feedback.analyze_feedback()\n",
        "\n",
        "    # ----- RLHF Integration Methods -----\n",
        "\n",
        "    def train_rlhf(self):\n",
        "        \"\"\"Train the RLHF system\"\"\"\n",
        "        result = self.feedback.run_rlhf_training_pipeline()\n",
        "        if result:\n",
        "            self.rlhf_trained = True\n",
        "            self.llm.toggle_rlhf(True)\n",
        "        return result\n",
        "\n",
        "    def has_enough_feedback(self):\n",
        "        \"\"\"Check if there's enough feedback to train RLHF\"\"\"\n",
        "        stats = self.feedback.get_feedback_stats()\n",
        "        return stats.get(\"total_feedback\", 0) >= self.feedback.min_samples\n",
        "\n",
        "    def toggle_rlhf(self, enable=True):\n",
        "        \"\"\"Toggle RLHF usage\"\"\"\n",
        "        if enable and not self.rlhf_trained:\n",
        "            if self.has_enough_feedback():\n",
        "                print(\"RLHF model not trained yet. Training now...\")\n",
        "                return self.train_rlhf()\n",
        "            else:\n",
        "                print(f\"Not enough feedback samples. Need at least {self.feedback.min_samples} samples.\")\n",
        "                return False\n",
        "\n",
        "        return self.llm.toggle_rlhf(enable)"
      ],
      "metadata": {
        "id": "Nkmm9UPYpeim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "UI & Run System\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "RptHb05dpjb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mit_rag_system():\n",
        "    # Initialize system with improved model\n",
        "    rag_system = MitRAGSystem(\n",
        "      collection_name=f\"collection_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
        "      model_name=\"google/flan-t5-base\",\n",
        "      embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "    )\n",
        "\n",
        "    # Output areas\n",
        "    doc_info_output = widgets.Output()\n",
        "    answer_output = widgets.Output()\n",
        "    analytics_output = widgets.Output()\n",
        "    rlhf_output = widgets.Output()\n",
        "\n",
        "    # Create upload button that uses file API directly\n",
        "    upload_button = widgets.Button(\n",
        "        description='Upload Document',\n",
        "        button_style='primary'\n",
        "    )\n",
        "\n",
        "    process_button = widgets.Button(\n",
        "        description='Process Document',\n",
        "        disabled=True,\n",
        "        button_style='primary'\n",
        "    )\n",
        "\n",
        "    # Text display to show uploaded filename\n",
        "    file_text = widgets.HTML(\"No file uploaded\")\n",
        "\n",
        "    # Store the uploaded file\n",
        "    uploaded_file = [None]\n",
        "\n",
        "    # Create query widgets\n",
        "    query_input = widgets.Text(\n",
        "        description='Question:',\n",
        "        placeholder='Ask a question about the document...',\n",
        "        disabled=True,\n",
        "        layout={'width': '80%'}\n",
        "    )\n",
        "\n",
        "    query_button = widgets.Button(\n",
        "        description='Search',\n",
        "        disabled=True,\n",
        "        button_style='success'\n",
        "    )\n",
        "\n",
        "    # RLHF toggle and button\n",
        "    rlhf_toggle = widgets.Checkbox(\n",
        "        value=False,\n",
        "        description='Use RLHF model',\n",
        "        disabled=True,\n",
        "        indent=False\n",
        "    )\n",
        "\n",
        "    rlhf_train_button = widgets.Button(\n",
        "        description='Train RLHF Model',\n",
        "        disabled=True,\n",
        "        button_style='warning'\n",
        "    )\n",
        "\n",
        "    # Create feedback widgets\n",
        "    feedback_widget = widgets.HBox([\n",
        "        widgets.Label(\"Rate:\"),\n",
        "        widgets.RadioButtons(\n",
        "            options=[('', 1), ('', 2), ('', 3), ('', 4), ('', 5)],\n",
        "            layout={'width': 'max-content'},\n",
        "            disabled=True\n",
        "        ),\n",
        "        widgets.Button(\n",
        "            description='Submit',\n",
        "            disabled=True,\n",
        "            button_style='info',\n",
        "            layout={'width': 'auto'}\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # Analytics button\n",
        "    analytics_button = widgets.Button(\n",
        "        description='Show Analytics',\n",
        "        disabled=False,\n",
        "        button_style='info'\n",
        "    )\n",
        "\n",
        "    # Store the current conversation ID\n",
        "    current_conversation_id = [None]\n",
        "\n",
        "    def on_upload_click(b):\n",
        "        try:\n",
        "            clear_output(wait=True)\n",
        "            display(widgets.HTML(\"<h2>MIT RAG System with Neural RLHF</h2>\"))\n",
        "            display(widgets.VBox([\n",
        "                widgets.HBox([upload_button, process_button]),\n",
        "                file_text\n",
        "            ]))\n",
        "            display(doc_info_output)\n",
        "            display(widgets.HTML(\"<h3>Ask a Question</h3>\"))\n",
        "            display(widgets.HBox([query_input, query_button, rlhf_toggle, rlhf_train_button]))\n",
        "            display(answer_output)\n",
        "            display(widgets.HBox([feedback_widget, analytics_button]))\n",
        "            display(analytics_output)\n",
        "            display(rlhf_output)\n",
        "\n",
        "            print(\"Please select a file in the pop-up dialog...\")\n",
        "            uploaded = files.upload()\n",
        "\n",
        "            if uploaded:\n",
        "                filename = next(iter(uploaded))\n",
        "                content = uploaded[filename]\n",
        "                file_text.value = f\"<b>Uploaded:</b> {filename} ({len(content)} bytes)\"\n",
        "                uploaded_file[0] = (filename, content)\n",
        "                process_button.disabled = False\n",
        "            else:\n",
        "                file_text.value = \"<b>Upload cancelled or failed</b>\"\n",
        "        except Exception as e:\n",
        "            file_text.value = f\"<b>Error:</b> {str(e)}\"\n",
        "\n",
        "    def on_process_click(b):\n",
        "        doc_info_output.clear_output()\n",
        "        with doc_info_output:\n",
        "            if not uploaded_file[0]:\n",
        "                print(\"Please upload a document first.\")\n",
        "                return\n",
        "\n",
        "            filename, content = uploaded_file[0]\n",
        "            print(f\"Processing: {filename}\")\n",
        "\n",
        "            success = rag_system.process_document(\n",
        "                file_content=content,\n",
        "                file_name=filename\n",
        "            )\n",
        "\n",
        "            if success:\n",
        "                print(\"\\n Document Information:\")\n",
        "                for key, value in rag_system.get_document_info().items():\n",
        "                    print(f\"  {key}: {value}\")\n",
        "                query_input.disabled = False\n",
        "                query_button.disabled = False\n",
        "            else:\n",
        "                print(\" Document processing failed.\")\n",
        "\n",
        "    def on_query_click(b):\n",
        "        answer_output.clear_output()\n",
        "        with answer_output:\n",
        "            query = query_input.value\n",
        "            if not query:\n",
        "                print(\"Please enter a question.\")\n",
        "                return\n",
        "\n",
        "            print(f\"Query: '{query}'\")\n",
        "\n",
        "            # Use RLHF if toggled on\n",
        "            use_rlhf = rlhf_toggle.value\n",
        "            if use_rlhf:\n",
        "                print(\"Using RLHF-enhanced model for answering...\")\n",
        "\n",
        "            answer, results, conversation_id = rag_system.answer_question(query, top_k=3, use_rlhf=use_rlhf)\n",
        "\n",
        "            current_conversation_id[0] = conversation_id\n",
        "            feedback_widget.children[1].disabled = False\n",
        "            feedback_widget.children[2].disabled = False\n",
        "\n",
        "            print(\"\\nAnswer:\")\n",
        "            print(answer)\n",
        "\n",
        "            print(\"\\nRetrieved passages:\")\n",
        "            for i, result in enumerate(results[:2]):  # Show only top 2 for brevity\n",
        "                print(f\"  Passage {i+1} (Score: {result['score']:.4f})\")\n",
        "                print(f\"  {result['text'][:150]}...\" if len(result['text']) > 150 else f\"  {result['text']}\")\n",
        "\n",
        "    def on_feedback_submit(b):\n",
        "        if current_conversation_id[0] is None:\n",
        "            return\n",
        "\n",
        "        rating = feedback_widget.children[1].value\n",
        "        success = rag_system.add_feedback(current_conversation_id[0], rating)\n",
        "\n",
        "        feedback_widget.children[1].disabled = True\n",
        "        feedback_widget.children[2].disabled = True\n",
        "\n",
        "        with answer_output:\n",
        "            print(f\"\\n Thank you for your feedback! (Rating: {rating})\")\n",
        "\n",
        "            # Check if we have enough feedback for RLHF\n",
        "            if rag_system.has_enough_feedback():\n",
        "                rlhf_train_button.disabled = False\n",
        "                print(f\"\\nYou've provided enough feedback to train the RLHF model! Click 'Train RLHF Model' to begin training.\")\n",
        "\n",
        "    def on_analytics_click(b):\n",
        "        analytics_output.clear_output()\n",
        "        with analytics_output:\n",
        "            print(\"System Analytics and Feedback\")\n",
        "            print(\"=============================\")\n",
        "\n",
        "            # Plot feedback stats\n",
        "            stats = rag_system.feedback.get_feedback_stats()\n",
        "            if stats and stats.get(\"total_feedback\", 0) > 0:\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                ratings = list(stats[\"rating_distribution\"].keys())\n",
        "                counts = list(stats[\"rating_distribution\"].values())\n",
        "                plt.bar(ratings, counts)\n",
        "                plt.title('Feedback Ratings')\n",
        "                plt.xlabel('Rating')\n",
        "                plt.ylabel('Count')\n",
        "                plt.xticks(range(1, 6))\n",
        "                plt.show()\n",
        "\n",
        "                print(f\"Average Rating: {stats['average_rating']:.2f} ({stats['total_feedback']} ratings)\")\n",
        "\n",
        "                # RLHF status\n",
        "                min_samples = rag_system.feedback.min_samples\n",
        "                if stats.get(\"total_feedback\", 0) >= min_samples:\n",
        "                    print(f\" Enough feedback for RLHF training ({stats['total_feedback']}/{min_samples})\")\n",
        "                else:\n",
        "                    print(f\" Need more feedback for RLHF training ({stats['total_feedback']}/{min_samples})\")\n",
        "            else:\n",
        "                print(\"No feedback data available yet.\")\n",
        "\n",
        "            # Show RLHF analysis\n",
        "            analysis = rag_system.analyze_feedback()\n",
        "            if isinstance(analysis, dict) and \"insights\" in analysis:\n",
        "                print(\"\\nInsights from feedback:\")\n",
        "                for i, insight in enumerate(analysis[\"insights\"]):\n",
        "                    print(f\"{i+1}. {insight}\")\n",
        "            else:\n",
        "                print(\"\\nNot enough feedback data for detailed analysis.\")\n",
        "\n",
        "    def on_rlhf_train(b):\n",
        "        rlhf_output.clear_output()\n",
        "        with rlhf_output:\n",
        "            print(\" Training RLHF Model\")\n",
        "            print(\"=====================\")\n",
        "            print(\"This may take a few minutes. Training neural reward model and fine-tuning LLM with PPO...\")\n",
        "\n",
        "            success = rag_system.train_rlhf()\n",
        "\n",
        "            if success:\n",
        "                print(\"\\n RLHF model trained successfully!\")\n",
        "                rlhf_toggle.disabled = False\n",
        "                rlhf_toggle.value = True  # Enable RLHF by default after training\n",
        "\n",
        "                print(\"\\nThe system will now use the RLHF model by default.\")\n",
        "                print(\"You can toggle between the base model and RLHF model using the checkbox.\")\n",
        "            else:\n",
        "                print(\"\\n RLHF model training failed. See error messages above.\")\n",
        "\n",
        "    def on_rlhf_toggle(change):\n",
        "        if change['new'] == True:  # Trying to enable RLHF\n",
        "            if not rag_system.rlhf_trained:\n",
        "                with rlhf_output:\n",
        "                    rlhf_output.clear_output()\n",
        "                    print(\"Enabling RLHF...\")\n",
        "                    if not rag_system.toggle_rlhf(True):\n",
        "                        print(\" Could not enable RLHF. Need to train model first.\")\n",
        "                        rlhf_toggle.value = False\n",
        "                    else:\n",
        "                        print(\" RLHF model enabled\")\n",
        "        else:\n",
        "            rag_system.toggle_rlhf(False)\n",
        "            with rlhf_output:\n",
        "                rlhf_output.clear_output()\n",
        "                print(\" Standard model enabled (RLHF disabled)\")\n",
        "\n",
        "    # Connect callbacks\n",
        "    upload_button.on_click(on_upload_click)\n",
        "    process_button.on_click(on_process_click)\n",
        "    query_button.on_click(on_query_click)\n",
        "    feedback_widget.children[2].on_click(on_feedback_submit)\n",
        "    analytics_button.on_click(on_analytics_click)\n",
        "    rlhf_train_button.on_click(on_rlhf_train)\n",
        "    rlhf_toggle.observe(on_rlhf_toggle, names='value')\n",
        "\n",
        "    # Display UI\n",
        "    display(widgets.HTML(\"<h2>MIT RAG System with Neural RLHF</h2>\"))\n",
        "    display(widgets.VBox([\n",
        "        widgets.HBox([upload_button, process_button]),\n",
        "        file_text\n",
        "    ]))\n",
        "    display(doc_info_output)\n",
        "    display(widgets.HTML(\"<h3>Ask a Question</h3>\"))\n",
        "    display(widgets.HBox([query_input, query_button, rlhf_toggle, rlhf_train_button]))\n",
        "    display(answer_output)\n",
        "    display(widgets.HBox([feedback_widget, analytics_button]))\n",
        "    display(analytics_output)\n",
        "    display(rlhf_output)\n",
        "\n",
        "    return rag_system\n",
        "\n",
        "# Run the system\n",
        "mit_rag = create_mit_rag_system()"
      ],
      "metadata": {
        "id": "ApoXX7V3poLL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
